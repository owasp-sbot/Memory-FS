# Memory-FS - Architecture and How to Use It (LLM Brief)
**Version**: v0.29.0  
**Updated**: September 22, 2025
**PyPI Package**: https://pypi.org/project/memory-fs/

## Executive Summary

Memory-FS is a sophisticated file system abstraction layer that provides content-addressable storage with multiple organizational strategies, backend-agnostic storage, and complete type safety through OSBot-Utils Type_Safe. It separates the concerns of WHERE files are stored (Storage_FS) from HOW they are organized (Path Handlers), enabling flexible, scalable file management patterns suitable for everything from simple key-value storage to complex versioned temporal systems.

The fundamental problem Memory-FS solves is that traditional file systems force you to make storage decisions early and couple your application logic to those decisions. If you start with local files and later need S3, you're rewriting code. If you begin with simple storage but later need versioning, you're restructuring everything. Memory-FS was created to break this coupling - you write your application logic once, and storage backends and organizational strategies can be swapped independently without touching your business logic.

Unlike traditional file systems that couple storage location with organization, Memory-FS allows you to mix and match storage backends (S3, SQLite, local disk, memory, ZIP) with organizational patterns (latest-only, temporal, versioned, semantic) without changing your application code. Every operation is type-safe, preventing entire categories of bugs related to file operations, data corruption, and type confusion.

This design emerged from real-world needs in distributed systems where the same data might be cached in memory for speed, persisted to S3 for durability, and organized both temporally (for audit trails) and as latest-only (for quick access). Rather than writing different code for each scenario, Memory-FS provides a unified interface that adapts to your needs.

## Core Architecture

### The Problem That Led to the Three-Layer Design

Before Memory-FS, developers faced a recurring challenge: file operations were tightly coupled to their storage medium. Code written for local files couldn't easily switch to S3. SQLite-based storage required different logic than file-based storage. Even worse, changing how files were organized (adding versioning, temporal organization) meant rewriting the storage layer entirely.

This coupling created several pain points:
- **Testing was expensive**: Tests against S3 were slow and required credentials
- **Migration was risky**: Moving from one storage system to another meant touching every file operation
- **Flexibility was limited**: You couldn't easily have the same file in multiple places (like cache + persistent storage)
- **Organization was fixed**: Adding versioning or temporal organization required massive refactoring

### Three-Layer Design

Memory-FS implements a clean three-layer architecture where each layer has a single responsibility, solving the coupling problem elegantly:

```
┌─────────────────────────────────────────┐
│         Application Layer               │
│     (Memory_FS + Helper Classes)        │
├─────────────────────────────────────────┤
│         File Operations Layer           │
│        (File_FS + Actions)              │
├─────────────────────────────────────────┤
│         Storage Backend Layer           │
│   (Storage_FS Implementations)          │
└─────────────────────────────────────────┘
```

**Layer 1 - Application Layer**: This is where your code interacts with Memory-FS. The `Memory_FS` class acts as the orchestrator, combining storage backends with path handlers to create your desired file system behavior. Helper classes like `Memory_FS__Latest` and `Memory_FS__Temporal` provide pre-configured patterns for common use cases. This layer abstracts all the complexity below it - you just work with files by ID without worrying about paths, storage details, or serialization.

**Layer 2 - File Operations Layer**: The `File_FS` class and its associated actions (`File_FS__Create`, `File_FS__Update`, etc.) handle the complete lifecycle of individual files. This layer manages three critical aspects of each file: the actual content, the configuration (file type, paths, strategies), and the metadata (hash, size, timestamp, custom data). By separating these concerns, Memory-FS can track file history, verify integrity, and support complex operations like versioning without the storage layer needing to understand these concepts.

**Layer 3 - Storage Backend Layer**: The foundation where bytes actually get persisted. Each `Storage_FS` implementation (Memory, SQLite, S3, etc.) provides the same interface but with radically different performance characteristics and capabilities. The beauty of this design is that the upper layers don't care whether data is stored in RAM, on disk, or in the cloud - they just call `file__save()` and the appropriate backend handles the details.

This separation was crucial for solving real production challenges:
- **Backend Independence**: A service that starts with SQLite in development can seamlessly move to S3 in production by changing one line of configuration
- **Strategy Flexibility**: You can add versioning to an existing system without migrating data - just add a versioned path handler
- **Testing Simplicity**: Tests run 100x faster using memory storage, while production uses S3 - same code, different backend
- **Performance Optimization**: Each backend is optimized for its medium - SQLite uses transactions, S3 uses multipart uploads, memory uses dictionaries

### Component Relationships

The key strength of Memory-FS is in how these components work together. Here's what actually happens when you work with a file:

```python
# Step 1: Create a Memory_FS instance (the orchestrator)
memory_fs = Memory_FS()

# Step 2: Tell it WHERE to store files (the backend)
memory_fs.set_storage(storage_fs)          # Could be S3, SQLite, memory, etc.

# Step 3: Tell it HOW to organize files (the strategy)
memory_fs.add_handler(path_handler)        # Latest, temporal, versioned, etc.

# Step 4: Work with files using a clean interface
file = memory_fs.file(file_id="doc-123")   # Returns File_FS instance
file.create(data)                          # Magic happens here!
```

What happens in that `file.create(data)` call is sophisticated:
1. The file type serializer converts your data to bytes (JSON, text, binary, etc.)
2. Each path handler generates its paths (latest/doc-123.json, 2025/09/22/doc-123.json, etc.)
3. The File_FS creates three components: content, config, and metadata files
4. The storage backend saves all components to its medium
5. References are updated for quick lookup by hash or ID

This orchestration means you can have the same logical file stored in multiple places (latest + temporal), in different formats (JSON for API, binary for efficiency), with complete metadata tracking, all without writing storage-specific code.

## Storage Backends (WHERE Files Are Stored)

### Understanding the Storage_FS Philosophy

The Storage_FS interface is deliberately minimal - just seven methods that any storage system can implement. This minimalism is a feature, not a limitation. By keeping the interface small, Memory-FS ensures that ANY storage system can become a backend, from exotic databases to cloud storage services to custom implementations.

The key insight was recognizing that at its core, every storage system does the same things: save bytes, retrieve bytes, check existence, and delete. By standardizing on these operations, Memory-FS can work with any storage system that exists or might be invented in the future.

### Storage_FS Interface

All storage backends implement the same interface, ensuring complete interchangeability. This interface was carefully designed to be the minimal set of operations needed for a complete file system:

```python
class Storage_FS(Type_Safe):
    def file__bytes(self, path: Safe_Str__File__Path) -> bytes: pass
    def file__delete(self, path: Safe_Str__File__Path) -> bool: pass
    def file__exists(self, path: Safe_Str__File__Path) -> bool: pass
    def file__json(self, path: Safe_Str__File__Path) -> dict: pass
    def file__save(self, path: Safe_Str__File__Path, data: bytes) -> bool: pass
    def file__str(self, path: Safe_Str__File__Path) -> str: pass
    def files__paths(self) -> List[Safe_Str__File__Path]: pass
```

### Available Storage Backends

Each storage backend was created to solve specific problems encountered in production systems. Understanding why each exists helps you choose the right one for your use case.

#### 1. Storage_FS__Memory (In-Memory)

The in-memory backend was the first created and remains the most important for development and testing. It stores everything in Python dictionaries, providing sub-millisecond operations with zero external dependencies.

```python
# Fastest, perfect for testing and caching
memory_fs = Memory_FS()
memory_fs.set_storage__memory()

# Why this backend exists:
# - Unit tests need isolation and speed (100x faster than disk)
# - Caching layers need microsecond response times
# - Temporary processing doesn't need persistence
# - Development environments need zero setup

# Characteristics:
# - Sub-millisecond operations (typically 10-100 microseconds)
# - Data lost on process exit
# - No size limits (bound by RAM)
# - Thread-safe operations via Type_Safe
# - Perfect atomicity (dictionary operations are atomic in Python)

# Real-world use case:
# A microservice that processes images might use memory storage
# for temporary files during processing, avoiding disk I/O entirely
```

The memory backend's strength is its simplicity - it's just a dictionary mapping paths to bytes. This makes it perfect for understanding Memory-FS concepts without the complexity of external systems. It's also invaluable for testing - your test suite can create, modify, and delete thousands of files per second without touching disk.

#### 2. Storage_FS__Local_Disk (File System)

The local disk backend bridges Memory-FS with traditional file systems. It was created because many applications need to work with existing file-based tools and systems while gaining Memory-FS benefits like type safety and path handlers.

```python
# Traditional file system storage with Memory-FS benefits
memory_fs = Memory_FS()
memory_fs.set_storage__local_disk(root_path="/data/storage")

# Why this backend exists:
# - Legacy systems integration (work with existing file-based tools)
# - Development environments (inspect files with regular tools)
# - Small deployments (single server applications)
# - File system features (symbolic links, permissions, etc.)

# Characteristics:
# - Persistent across restarts
# - OS file system limits apply (filename length, path depth)
# - Automatic parent directory creation (no more makedirs)
# - Virtual path support (logical paths independent of physical location)
# - Native OS caching (kernel page cache)

# Real-world use case:
# A documentation system that generates static files for a web server.
# Memory-FS provides versioning and organization while nginx serves
# the actual files from disk.
```

The local disk backend is more sophisticated than it appears. It handles path normalization across operating systems, creates parent directories automatically, and provides virtual paths so your logical file organization doesn't have to match the physical layout. This is crucial when migrating existing applications to Memory-FS.

#### 3. Storage_FS__Sqlite (Database)

SQLite storage was added to solve a specific problem: applications that need file storage with database features like transactions, indexing, and atomic operations. It stores files as BLOBs in a SQLite database, providing a middle ground between simple files and full database systems.

```python
# SQLite with BLOB storage - best of both worlds
memory_fs = Memory_FS()
memory_fs.set_storage__sqlite(db_path="/data/storage.db", in_memory=False)

# Or in-memory SQLite (faster than disk, structured unlike dict)
memory_fs.set_storage__sqlite(in_memory=True)

# Why this backend exists:
# - Single file deployment (entire file system in one .db file)
# - ACID transactions (atomic operations across multiple files)
# - Indexed lookups (find files by metadata efficiently)
# - Embedded systems (no external database needed)
# - Backup simplicity (copy one file to backup everything)

# Characteristics:
# - ACID compliance (transactions, rollback on failure)
# - Single file storage (easy to backup, move, or embed)
# - Indexed lookups (O(log n) searches by path)
# - Built-in vacuum optimization (reclaim deleted space)
# - In-memory option (structured storage without disk)
# - SQL query capability (though not exposed directly)

# Real-world use case:
# An IoT device that needs reliable file storage with limited
# resources. SQLite provides durability and atomicity without
# requiring a separate database server.
```

The SQLite backend includes useful features: it maintains indexes for fast lookups, uses transactions for atomicity, and can even run entirely in memory for testing. The table structure (path, data, created_at, updated_at) provides built-in audit capabilities that other backends would need to implement separately.

#### 4. Storage_FS__Zip (Compressed Archive)

The ZIP backend emerged from the need to package and transfer entire file systems efficiently. It stores files in a compressed ZIP archive, either in memory or on disk, providing compression and portability.

```python
# ZIP file storage with optional disk persistence
memory_fs = Memory_FS()
memory_fs.set_storage__zip(zip_path="/data/archive.zip", in_memory=False)

# Why this backend exists:
# - Data transfer (send entire file systems over network)
# - Backup/restore (compressed archives of complete state)
# - Deployment packages (ship code + data together)
# - Storage cost reduction (automatic compression)
# - Archive compliance (immutable historical records)

# Characteristics:
# - Automatic compression (typically 50-90% size reduction for text)
# - Single archive file (portable, email-able)
# - Import/export support (merge multiple archives)
# - In-memory or disk-backed (flexibility for different use cases)
# - Standard ZIP format (compatible with all ZIP tools)
# - Incremental updates (add/remove files without rewriting entire archive)

# Real-world use case:
# A reporting system that generates monthly archives. Each month's
# data is stored in a ZIP backend, compressed and ready for long-term
# storage or transfer to clients.
```

The ZIP backend is surprisingly versatile. It can run entirely in memory for temporary compressed storage, or persist to disk for archives. The ability to import and export ZIP files makes it perfect for data exchange between systems. The implementation uses Python's zipfile module efficiently, updating archives incrementally rather than rewriting the entire file.

#### 5. Storage_FS__S3 (Cloud Object Storage)
```python
# S3-compatible object storage for cloud scale
from memory_fs.storage_fs.providers.Storage_FS__S3 import Storage_FS__S3

storage_s3 = Storage_FS__S3(bucket_name="my-bucket", prefix="data/")
memory_fs = Memory_FS()
memory_fs.set_storage(storage_s3)

# Why this backend exists:
# - Cloud-native applications (runs where your app runs)
# - Infinite scale (no storage limits)
# - Geographic distribution (multi-region replication)
# - Cost optimization (pay only for what you use)
# - Integration ecosystem (works with entire AWS toolchain)

# Characteristics:
# - Infinite scale (billions of objects, petabytes of data)
# - 5TB per object limit (large enough for any file)
# - Eventual consistency (may need slight delays after writes)
# - Network latency (typically 10-100ms per operation)
# - Built-in redundancy (99.999999999% durability)
# - Lifecycle policies (automatic archival, deletion)

# Real-world use case:
# A content management system serving global users. Files are stored
# in S3 with CloudFront CDN for distribution. Memory-FS handles the
# organization while S3 provides the scale and reliability.
```

The S3 backend is where Memory-FS truly shines in production. It abstracts away all the complexity of working with S3 (authentication, error handling, retries) while providing the same simple interface as other backends. The prefix support allows multiple Memory-FS instances to share a single bucket safely.

## Path Handlers (HOW Files Are Organized)

### The Philosophy Behind Path Handlers

Path handlers solve a fundamental problem: different data needs different organizational strategies, but traditional file systems force you to pick one. Configuration files need quick access to the latest version. Audit logs need temporal organization for compliance. Documents need version history for rollback. With traditional approaches, you'd need three different storage systems or complex application logic.

Memory-FS's breakthrough was recognizing that HOW you organize files is independent of WHERE you store them. A path handler is simply a strategy for generating file paths based on your needs. You can combine multiple handlers, and each will create its own copy of the file at its generated path. This means you can have your cake and eat it too - the latest version for quick access AND a complete temporal history for auditing.

The path handler system is also extensible. While Memory-FS provides common patterns (latest, temporal, versioned), you can create custom handlers for domain-specific needs. This flexibility means Memory-FS can adapt to any organizational requirement without framework changes.

### Path Handler Types

Each path handler implements a specific organizational pattern that emerged from real-world needs:

#### 1. Path__Handler__Latest

The Latest handler maintains only the most recent version in a predictable location. This pattern emerged from the most common file system need: "I just want the current version."

```python
memory_fs = Memory_FS()
memory_fs.set_storage__memory()
memory_fs.add_handler__latest()

# Files stored at: latest/{file_id}.{ext}

# Why this pattern exists:
# Configuration files, user preferences, and current state all share a
# common need: you usually only care about the current version. Keeping
# historical versions would waste space and complicate access.

# How it works internally:
# 1. File_FS asks Latest handler for paths
# 2. Handler returns: ["latest/"]
# 3. File saved at: latest/document.json
# 4. Updates overwrite the previous version
# 5. No history maintained (unless combined with other handlers)

# Real-world scenario:
# Application configuration that changes rarely but needs fast access.
# The config is always at "latest/config.json" - no searching, no
# version selection, just simple direct access.

# Perfect for: Current state, configuration files, single version needs
```

The Latest handler is deceptively simple but solves 80% of file storage needs. By putting everything in a "latest" folder, it provides a predictable, stable location for files regardless of when they were updated. This predictability is crucial for systems that need to know exactly where files are without querying databases or maintaining mappings.

#### 2. Path__Handler__Temporal  

The Temporal handler organizes files by timestamp, creating a natural time-based hierarchy. This pattern was inspired by logging systems that need to organize millions of files by when they were created.

```python
memory_fs = Memory_FS()
memory_fs.set_storage__memory()
memory_fs.add_handler__temporal(
    areas=["logs", "application"],
    time_path_pattern="%Y/%m/%d/%H"  # Year/Month/Day/Hour
)

# Files stored at: 2025/09/22/10/logs/application/{file_id}.{ext}

# Why this pattern exists:
# Audit requirements often mandate keeping records organized by time.
# Temporal organization makes it trivial to find all files from a specific
# time period, implement retention policies, or archive old data.

# How it works internally:
# 1. Handler captures current timestamp when path is generated
# 2. Formats time using strftime pattern (customizable)
# 3. Appends area hierarchy if specified
# 4. Creates path like: 2025/09/22/10/logs/application/
# 5. Each save creates a new file (no overwrites)

# Areas explanation:
# Areas are hierarchical categories that further organize files.
# areas=["logs", "application", "auth"] creates: 
# 2025/09/22/10/logs/application/auth/login-attempt.json
# This allows filtering by both time AND category.

# Real-world scenario:
# Compliance logging for financial transactions. Every transaction
# is stored with a temporal path, making it easy to retrieve all
# transactions from a specific hour, day, or month for auditing.

# Perfect for: Audit logs, time-series data, compliance, historical records
```

The temporal handler's power comes from its natural alignment with how humans think about time. Finding "all logs from yesterday at 3 PM" becomes a simple path traversal. The configurable time pattern means you can organize by second, minute, hour, day, or any other time unit that makes sense for your data volume and access patterns.

#### 3. Path__Handler__Versioned

The Versioned handler maintains explicit version numbers, providing clear version progression and easy rollback capabilities. This pattern came from document management systems where tracking version history is critical.

```python
memory_fs = Memory_FS()
memory_fs.set_storage__memory()
handler = memory_fs.add_handler__versioned()

file = memory_fs.file("document")
file.create("version 1")           # Saved at: v1/document.txt

handler.increment_version()         # Now at v2
file.create("version 2")           # Saved at: v2/document.txt

handler.set_version(10)            # Jump to v10
file.create("version 10")          # Saved at: v10/document.txt

# Why this pattern exists:
# Documents, code, and configurations often need explicit version control.
# While git handles code versioning, data files need their own versioning
# strategy that's independent of the code repository.

# How it works internally:
# 1. Handler maintains current_version counter (starting at 1)
# 2. Generates path with version prefix: v{N}/
# 3. increment_version() bumps the counter
# 4. set_version() allows jumping to specific versions
# 5. All versions preserved (true version history)

# Version management:
# - Increment: Move to next version sequentially
# - Set: Jump to specific version (useful for restoring)
# - Current: Always know which version is active
# - History: All previous versions remain accessible

# Real-world scenario:
# Legal document management where every contract revision must be
# preserved. Version 1 might be the initial draft, v2 has client
# changes, v3 is the signed version. All are preserved and accessible.

# Perfect for: Document versioning, rollback capability, change tracking
```

The versioned handler provides explicit, human-readable version numbers unlike git's SHA hashes. This makes it perfect for user-facing versioning where people need to understand version relationships (v2 comes after v1) without technical knowledge.

#### 4. Path__Handler__Custom

The Custom handler allows arbitrary path generation for domain-specific needs. This pattern acknowledges that not all organizational needs fit into pre-defined patterns.

```python
memory_fs = Memory_FS()
memory_fs.set_storage__memory()
memory_fs.add_handler__custom(
    custom_path="projects/alpha/2025/Q1"
)

# Files stored at: projects/alpha/2025/Q1/{file_id}.{ext}

# Why this pattern exists:
# Real-world systems often have domain-specific organization requirements
# that don't fit temporal or versioned patterns. Project hierarchies,
# geographic organization, or department structures need custom paths.

# How it works internally:
# 1. Uses the exact custom_path provided
# 2. No automatic generation or modification
# 3. Can include any valid path characters
# 4. Combines with prefix/suffix if specified
# 5. Static path (doesn't change over time)

# Use cases for custom paths:
# - Organization by project: "projects/apollo/phase1/"
# - Geographic distribution: "regions/us-east/california/"
# - Department hierarchy: "finance/accounts-payable/2025/"
# - Client segregation: "clients/acme-corp/contracts/"
# - Environment separation: "environments/production/configs/"

# Real-world scenario:
# Multi-tenant SaaS where each client's data must be completely isolated.
# Custom handler creates paths like "tenants/client-123/data/" ensuring
# complete separation at the storage level.

# Perfect for: Domain-specific organization, legacy systems, complex hierarchies
```

The custom handler is the escape hatch that makes Memory-FS flexible enough for any organization requirement. It's often used in migration scenarios where you need to maintain compatibility with existing path structures.

### Combining Path Handlers

The true power of Memory-FS emerges when you combine multiple handlers. This creates files in multiple locations simultaneously, solving complex requirements elegantly:

```python
# Example: Latest + Temporal for production systems
memory_fs = Memory_FS()
memory_fs.set_storage__s3(bucket="prod-data")
memory_fs.add_handler__latest()      # Quick access copy
memory_fs.add_handler__temporal()    # Historical record

file = memory_fs.file("config")
file.create({"version": "1.0"})

# This single create() call generates multiple files:
# - latest/config.json (for fast current access)
# - 2025/09/22/10/config.json (for audit trail)

# Why combine handlers?
# Different stakeholders need different views of the same data:
# - Operations needs the latest configuration (latest handler)
# - Compliance needs historical records (temporal handler)
# - Support needs version history (versioned handler)
# By combining handlers, one write satisfies all needs.
```

The combination pattern is particularly powerful because it eliminates the traditional trade-off between access speed and historical preservation. You don't have to choose between fast access to current data and maintaining an audit trail - you get both automatically.

Consider a real production scenario: a trading system that needs microsecond access to current prices (latest), regulatory compliance requiring all price updates be preserved (temporal), and the ability to reconstruct market state at any point in time (versioned). Traditional approaches would require complex application logic to maintain these three views. With Memory-FS, you simply add three handlers and the framework handles the complexity.

## Helper Classes for Common Patterns

The helper classes aren't just convenience wrappers - they encode proven patterns that solve specific problems. Each represents a battle-tested combination of storage and organization strategies that emerged from real-world use cases.

### Understanding Helper Class Philosophy

Helper classes were created after observing that certain combinations of handlers appeared repeatedly across different applications. Rather than requiring every developer to rediscover these patterns, Memory-FS provides pre-configured classes that embody best practices. These helpers also serve as examples of how to create your own domain-specific file system classes.

### Memory_FS__Latest

The Latest helper is the simplest and most commonly used pattern. It maintains only the current version of each file, perfect for state management, caching, and configuration.

```python
from memory_fs.helpers.Memory_FS__Latest import Memory_FS__Latest

fs = Memory_FS__Latest()
fs.set_storage__memory()

file = fs.file("user-123")
file.create({"name": "Alice", "preferences": {"theme": "dark"}})
# Stored at: latest/user-123.json

# Why this helper exists:
# The vast majority of file operations only care about current state.
# User preferences, application settings, cache entries - they all
# follow the pattern of "keep only the latest version."

# What it provides beyond basic Memory_FS:
# 1. Pre-configured with Latest handler
# 2. Exposes handler as .handler__latest for direct manipulation
# 3. Encodes the "single version" pattern explicitly
# 4. Makes intent clear in code: "This uses latest-only storage"

# Real-world example:
# A web application's session storage. Each user session is stored
# as a file, updated on every request, with only the current state
# maintained. Historical session states aren't needed, so Latest
# pattern is perfect.

# When to use:
# - Configuration management
# - Cache implementations
# - Current state storage
# - Any scenario where history isn't needed
```

### Memory_FS__Temporal

The Temporal helper implements pure time-based organization, essential for append-only systems like logging, event sourcing, and audit trails.

```python
from memory_fs.helpers.Memory_FS__Temporal import Memory_FS__Temporal

fs = Memory_FS__Temporal(areas=["events", "user"])
fs.set_storage__sqlite()

file = fs.file("login-event")
file.create({"user": "alice", "timestamp": "2025-09-22T10:30:00Z", "ip": "192.168.1.1"})
# Stored at: 2025/09/22/10/events/user/login-event.json

# Why this helper exists:
# Compliance and debugging often require chronological organization.
# When investigating issues, being able to see "everything that happened
# at 3 PM yesterday" is invaluable. Temporal organization makes this natural.

# The areas parameter:
# Areas create categorical hierarchies within time periods.
# areas=["security", "auth", "login"] creates a path like:
# 2025/09/22/10/security/auth/login/event.json
# This allows queries like "all security events from today" or
# "all login attempts from this hour"

# Implementation details:
# - Uses current time when generating paths (not modifiable)
# - Default pattern is %Y/%m/%d/%H (year/month/day/hour)
# - Can customize granularity based on volume
# - Each write creates a new file (never overwrites)

# Real-world example:
# API request logging for a high-traffic service. Every request
# is logged with temporal organization, making it easy to:
# - Find all requests from a specific hour
# - Archive old logs by simply moving date folders
# - Implement retention policies (delete folders older than 30 days)
# - Correlate events across services by timestamp

# When to use:
# - Audit logging
# - Event sourcing
# - Time-series data
# - Any append-only data pattern
```

### Memory_FS__Latest_Temporal

This helper combines the two most common patterns: maintaining current state while preserving history. It's the Swiss Army knife of Memory-FS helpers.

```python
from memory_fs.helpers.Memory_FS__Latest_Temporal import Memory_FS__Latest_Temporal

fs = Memory_FS__Latest_Temporal()
fs.set_storage__s3(bucket="prod-data")

file = fs.file("api-response", file_key="order-status")
file.create({"status": "pending", "items": 3, "total": 99.99})
# Creates BOTH:
# - latest/api-response.json (current status)
# - 2025/09/22/10/api-response.json (historical record)

# Why this helper exists:
# Production systems often need both fast access to current state
# AND historical records for debugging/compliance. This helper
# eliminates the traditional trade-off between speed and history.

# How the dual-write works:
# 1. Single create() call triggers both handlers
# 2. Latest handler generates: latest/
# 3. Temporal handler generates: 2025/09/22/10/
# 4. Storage backend saves to both locations
# 5. Updates overwrite latest but create new temporal entries

# Cost considerations:
# - Storage: 2x (one copy in latest, one in temporal)
# - Write performance: 2x operations (usually negligible)
# - Read performance: Optimal (latest for current, temporal for history)
# - Maintenance: Automatic (no synchronization needed)

# Real-world example:
# Inventory management system where:
# - Warehouse workers need current inventory (latest/)
# - Accounting needs to reconstruct inventory at month-end (temporal/)
# - Both needs are met with a single write operation

# When to use:
# - State that changes over time but needs history
# - Configuration with audit requirements
# - Any data where "current" and "historical" are both valuable
```

### Memory_FS__Versioned

The Versioned helper provides explicit version control for documents and data files, similar to source control but for data.

```python
from memory_fs.helpers.Memory_FS__Versioned import Memory_FS__Versioned

fs = Memory_FS__Versioned()
fs.set_storage__local_disk("/data/versioned")

# Version 1
doc = fs.file("contract-ABC")
doc.create("Initial draft with basic terms")

# Version 2
fs.handler__versioned.increment_version()
doc.update("Revised with client feedback")

# Version 3
fs.handler__versioned.increment_version()
doc.update("Final version for signing")

# Files created:
# - v1/contract-ABC.txt
# - v2/contract-ABC.txt
# - v3/contract-ABC.txt

# Why this helper exists:
# Documents, contracts, and policies need clear version progression.
# Unlike temporal organization (when), versioning tracks logical
# progression (what changed and why).

# Version semantics:
# - Versions are sequential integers (1, 2, 3...)
# - Each version is immutable once created
# - Can jump versions (v3 to v10) for major changes
# - Version numbers have business meaning

# Real-world example:
# Legal document management where each contract revision must be
# preserved. Lawyers can reference "version 3 of the contract"
# knowing exactly which iteration they're discussing.

# When to use:
# - Document management
# - Configuration with rollback needs
# - Any data with meaningful version progression
```

## File Types and Serialization

### The Serialization Layer Philosophy

File types in Memory-FS aren't just about file extensions - they encode complete serialization strategies including encoding, content type, and validation rules. This abstraction emerged from the observation that files aren't just bags of bytes; they have structure, encoding requirements, and type-specific handling needs.

The serialization layer ensures that data round-trips perfectly: what you save is exactly what you get back, regardless of the storage backend or path handlers used. This is harder than it sounds - JSON serialization must handle special types, text files need encoding consistency, and binary data must remain untouched.

### Built-in File Types

Memory-FS includes carefully crafted file types for the most common use cases. Each type knows how to serialize and deserialize its content correctly:

#### JSON Files (Default)

JSON is the default because it's the lingua franca of modern applications. The JSON file type handles all the complexity of JSON serialization including special types, pretty printing, and encoding.

```python
file = memory_fs.file__json("config")
file.create({
    "setting": "value",
    "number": 42,
    "nested": {"deep": "structure"}
})
data = file.content()  # Returns dict, perfectly preserved

# What happens internally:
# 1. Python dict is serialized to JSON string with sorted keys
# 2. JSON string is encoded to UTF-8 bytes
# 3. Bytes are saved via storage backend
# 4. On retrieval, bytes decoded to string
# 5. String parsed back to Python dict

# Why JSON is default:
# - Universal format (every language can read it)
# - Human-readable (debugging is easier)
# - Preserves structure (nested data maintained)
# - Type-safe (no ambiguity in types)

# Key features:
# - Sorted keys for consistent hashing
# - Pretty printing (2-space indent) for readability
# - UTF-8 encoding (handles international characters)
# - Special type handling (dates, Decimal, etc.)
```

#### Text Files

Text files handle plain text with proper encoding, crucial for logs, documentation, and human-readable content.

```python
file = memory_fs.file__text("readme")
file.create("""
# Project Documentation
This handles multi-line text with proper encoding.
Supports UTF-8 by default for international characters: 你好 🌍
""")
content = file.content()  # Returns string with exact formatting

# What happens internally:
# 1. String is encoded to UTF-8 bytes
# 2. Bytes saved via storage backend
# 3. On retrieval, bytes decoded back to string
# 4. Original formatting preserved exactly

# Why text files matter:
# - Log files need line-by-line processing
# - Configuration files (INI, YAML) are text-based
# - Documentation is written in Markdown/text
# - Scripts and code are text files

# Encoding is critical:
# - UTF-8 handles 99% of use cases
# - Consistent encoding prevents mojibake
# - Explicit encoding avoids platform differences
```

#### Binary Files  

Binary files store raw bytes without modification, essential for images, compressed data, and proprietary formats.

```python
file = memory_fs.file__binary("image")
with open("photo.jpg", "rb") as f:
    image_bytes = f.read()
    
file.create(image_bytes)
retrieved_bytes = file.content()  # Returns exact bytes

# What happens internally:
# 1. Bytes are passed through unchanged
# 2. No encoding or transformation
# 3. Hash is computed on raw bytes
# 4. Retrieved exactly as stored

# Why binary preservation matters:
# - Images corrupt if even one byte changes
# - Compressed files need bit-perfect storage
# - Cryptographic data requires exact bytes
# - Proprietary formats can't be interpreted

# Use cases:
# - Media files (images, audio, video)
# - Compressed archives (ZIP, tar.gz)
# - Encrypted data
# - Binary protocols
```

#### Data Files (Type_Safe Objects)
```python
# Type_Safe objects serialize perfectly
class Config(Type_Safe):
    api_key: Safe_Str
    timeout: Safe_UInt

file = memory_fs.file__data("config")
config = Config(api_key="secret-key", timeout=30)
file.create(config)

# On retrieval, you get the exact Type_Safe object back
restored_config = file.content()
assert isinstance(restored_config, Config)
assert restored_config.timeout == 30

# What happens internally:
# 1. Type_Safe object calls .json() to serialize
# 2. JSON includes type information for reconstruction
# 3. On retrieval, Type_Safe.from_json() reconstructs object
# 4. All type safety guarantees maintained

# Why Data files are powerful:
# - Domain objects serialize without custom code
# - Type safety maintained through storage
# - Validation happens automatically on load
# - Complex nested structures handled perfectly

# Real use case:
# Storing validated configuration objects that must maintain
# their constraints even after being loaded from disk.
```

The Data file type showcases Memory-FS's deep integration with Type_Safe. Unlike traditional serialization where you lose type information, Data files preserve the complete type structure, enabling perfect round-trip serialization of complex domain objects.

### Custom File Types

Creating custom file types allows you to extend Memory-FS for domain-specific formats. Each file type encodes not just the extension, but the complete strategy for handling that type of content:

```python
from memory_fs.schemas.Schema__Memory_FS__File__Type import Schema__Memory_FS__File__Type
from memory_fs.schemas.Enum__Memory_FS__Serialization import Enum__Memory_FS__Serialization
from memory_fs.schemas.Enum__Memory_FS__File__Content_Type import Enum__Memory_FS__File__Content_Type

class Memory_FS__File__Type__CSV(Schema__Memory_FS__File__Type):
    name           = Safe_Str__Id("csv")
    content_type   = Enum__Memory_FS__File__Content_Type.CSV  # MIME type
    file_extension = Safe_Str__Id("csv")
    encoding       = Enum__Memory_FS__File__Encoding.UTF_8    # Text encoding
    serialization  = Enum__Memory_FS__Serialization.STRING    # How to serialize

# Use custom type
file = memory_fs.file("data", file_type=Memory_FS__File__Type__CSV)
csv_content = "name,age,city\nAlice,30,Boston\nBob,25,Seattle"
file.create(csv_content)

# Why custom file types matter:
# - Encoding is explicit (no guessing)
# - Content-Type enables proper HTTP headers
# - Serialization strategy is configurable
# - Validation can be type-specific
# - Extensions are consistent

# Example: Custom YAML type
class Memory_FS__File__Type__YAML(Schema__Memory_FS__File__Type):
    name           = Safe_Str__Id("yaml")
    content_type   = Enum__Memory_FS__File__Content_Type.YAML
    file_extension = Safe_Str__Id("yaml")
    encoding       = Enum__Memory_FS__File__Encoding.UTF_8
    serialization  = Enum__Memory_FS__Serialization.STRING
    
    # Could add custom serialization logic here
    def serialize_custom(self, data):
        import yaml
        return yaml.dump(data)
```

The file type system is extensible because real-world applications work with diverse formats: CSV for data export, YAML for configuration, XML for enterprise integration, Protocol Buffers for high-performance IPC. Each has unique requirements that the file type abstraction captures cleanly.

## File Operations via File_FS

### Understanding the File_FS Layer

File_FS is where the magic happens. It's the orchestration layer that coordinates between storage backends, path handlers, and file types to provide a simple, consistent interface for file operations. Every File_FS instance represents a logical file that might be physically stored in multiple locations.

### Core File Operations

The File_FS class provides a complete interface for file lifecycle management:

```python
# Get a file reference - this doesn't touch storage yet
file = memory_fs.file("document-123")

# Create - Write new file to all configured paths
paths_created = file.create({"title": "My Document", "content": "..."})
# Returns: ['latest/document-123.json', '2025/09/22/document-123.json']
# Why return paths? So you know exactly what was created where

# Exists - Check if file exists using configured strategy
if file.exists():
    # Strategy matters here:
    # - ANY: True if exists in any location
    # - ALL: True only if exists in all locations
    # - FIRST: True if exists in first location
    print("File found")

# Read - Load and deserialize content
content = file.content()  # Returns Python object, not raw bytes
# This is the deserialized data:
# - JSON files return dicts
# - Text files return strings
# - Binary files return bytes
# - Data files return Type_Safe objects

# Info - Get comprehensive file information
info = file.info()
# Returns: {
#     'exists': True,
#     'size': 1234,           # Size in bytes
#     'content_hash': 'abc...', # Content hash for verification
#     'timestamp': 1234567890, # Creation timestamp
#     'content_type': 'application/json'
# }

# Update - Modify existing file
paths_updated = file.update({"title": "Updated Document", "content": "new..."})
# Important: Update overwrites in 'latest' but creates new in 'temporal'
# This is why path handlers are powerful - same operation, different semantics

# Delete - Remove from all locations
paths_deleted = file.delete()
# Returns list of successfully deleted paths
# Note: Partial deletion is possible if some backends fail

# The consistency of these operations:
# - Same interface regardless of storage backend
# - Same interface regardless of path handlers
# - Same interface regardless of file type
# - Type safety throughout
```

### File Metadata

Metadata is a first-class concept in Memory-FS. Every file automatically maintains rich metadata that goes beyond basic file attributes:

```python
from memory_fs.schemas.Schema__Memory_FS__File__Metadata import Schema__Memory_FS__File__Metadata

# Metadata is created automatically
file = memory_fs.file("report")
file.create("Quarterly earnings report...")

# Access metadata
metadata = file.metadata()
print(metadata.content__hash)     # SHA-256 hash of content
print(metadata.content__size)     # Size in bytes
print(metadata.timestamp)          # Creation timestamp
print(metadata.tags)              # User-defined tags
print(metadata.data)              # Custom key-value data

# Update metadata (only the 'data' section is mutable)
file.metadata__update({
    "author": "Alice",
    "department": "Finance",
    "quarter": "Q3-2025",
    "reviewed": True
})

# Why metadata is separate from content:
# 1. Performance - Read metadata without loading large files
# 2. Search - Query by metadata without touching content
# 3. Integrity - Hash verification without trusting content
# 4. Audit - Timestamp and tags for compliance
# 5. Extension - Custom data without modifying schema

# Real-world use:
# A document management system can display file listings with
# author, size, and modification time without loading actual
# documents. Searches can filter by metadata tags without
# deserializing content. Integrity checks can verify hashes
# without trusting potentially corrupted data.

# Metadata schema:
# - content__hash: Cryptographic hash for integrity
# - content__size: File size for display/quotas
# - timestamp: Creation time (immutable)
# - tags: Set of classification tags
# - data: Arbitrary key-value pairs for app-specific needs
# - chain_hash: For blockchain-like verification (optional)
# - previous_version_path: Link to previous version (optional)
```

The metadata system solves real problems: How do you display file information without loading gigabytes of data? How do you verify file integrity? How do you add application-specific attributes without modifying the storage layer? Metadata provides clean solutions to all these challenges.

### File Paths and Names

Memory-FS uses a sophisticated naming convention that encodes multiple pieces of information:

```python
# Memory-FS naming pattern:
# {file_id}.{extension}           # Content file
# {file_id}.{extension}.config    # Configuration  
# {file_id}.{extension}.metadata  # Metadata

# Example files created:
document-123.json           # The actual JSON content
document-123.json.config    # File configuration (type, paths, strategy)
document-123.json.metadata  # File metadata (hash, size, timestamp, tags)

# Why three files?
# 1. Separation of concerns - data, config, and metadata have different lifecycles
# 2. Atomic operations - can update metadata without touching large content files
# 3. Performance - read config/metadata without loading content
# 4. Integrity - config is immutable, metadata is mutable, content is versioned

# How paths are generated:
file = memory_fs.file("report", file_key="monthly/sales")

# Each handler generates its paths:
# Latest:    latest/report.json
# Temporal:  2025/09/22/10/report.json
# Versioned: v1/report.json
# Custom:    projects/alpha/report.json

# The file_key parameter:
# Some handlers (like semantic) use file_key for additional organization
# Others ignore it completely
# This flexibility allows handlers to be simple or sophisticated

# Path sharding for scale:
# Large systems might have millions of files
# Flat directories become slow
# Memory-FS can add sharding:
# Instead of: files/document-123.json
# Sharded:    files/d/o/document-123.json
# This distributes files across subdirectories for better performance
```

The three-file pattern might seem like overhead, but it solves real problems. Imagine updating tags on a 1GB video file - with separated metadata, you only write 500 bytes instead of rewriting 1GB. Or checking if a file exists - just check for the config file instead of loading content.

## Real-World Usage Patterns

These patterns emerged from actual production use cases where Memory-FS solved complex requirements elegantly:

### Pattern 1: Configuration Management

```python
# Setup: Latest-only for current config
config_fs = Memory_FS__Latest()
config_fs.set_storage__local_disk("/etc/myapp")

# Create/update configuration
config_file = config_fs.file("app-config")
config_file.create({
    "database": {"host": "localhost", "port": 5432},
    "cache": {"ttl": 3600}
})

# Read configuration
if config_file.exists():
    config = config_file.content()
    db_host = config["database"]["host"]
```

### Pattern 2: Audit Logging

```python
# Setup: Temporal for compliance
audit_fs = Memory_FS__Temporal(areas=["audit", "financial"])
audit_fs.set_storage__s3(bucket="audit-logs")

# Log events with automatic timestamping
def log_transaction(transaction_id, details):
    audit_file = audit_fs.file(f"tx-{transaction_id}")
    audit_file.create({
        "transaction_id": transaction_id,
        "details": details,
        "logged_by": current_user(),
        "ip_address": request_ip()
    })

# Files organized by time: 2025/09/22/10/audit/financial/tx-12345.json
```

### Pattern 3: Document Versioning

```python
# Setup: Versioned for document management
docs_fs = Memory_FS__Versioned()
docs_fs.set_storage__sqlite(db_path="/data/documents.db")

# Save document versions
doc = docs_fs.file("contract-ABC")
doc.create("Initial draft")

docs_fs.handler__versioned.increment_version()
doc.update("Revised draft with changes")

docs_fs.handler__versioned.increment_version()
doc.update("Final version")

# Files at: v1/contract-ABC.txt, v2/contract-ABC.txt, v3/contract-ABC.txt
```

### Pattern 4: Cache with Fallback

```python
# Setup: Memory for cache, S3 for persistence
cache_fs = Memory_FS__Latest_Temporal()
cache_fs.set_storage__memory()  # Fast cache

persist_fs = Memory_FS__Latest_Temporal()
persist_fs.set_storage__s3(bucket="persistent-cache")

def get_cached_data(key):
    # Try cache first
    cache_file = cache_fs.file(key)
    if cache_file.exists():
        return cache_file.content()
    
    # Fall back to persistent storage
    persist_file = persist_fs.file(key)
    if persist_file.exists():
        data = persist_file.content()
        # Populate cache
        cache_file.create(data)
        return data
    
    return None
```

### Pattern 5: Multi-Region Replication

```python
# Setup: Multiple storage backends for redundancy
regions = {
    "us-east": Storage_FS__S3(bucket="data-us-east"),
    "eu-west": Storage_FS__S3(bucket="data-eu-west"),
    "ap-south": Storage_FS__S3(bucket="data-ap-south")
}

def replicated_save(file_id, data):
    results = {}
    for region, storage in regions.items():
        fs = Memory_FS()
        fs.set_storage(storage)
        fs.add_handler__latest()
        
        file = fs.file(file_id)
        paths = file.create(data)
        results[region] = paths
    
    return results
```

## Advanced Features

### Content Addressing with Hashing

Memory-FS automatically generates content hashes for deduplication and verification:

```python
file1 = memory_fs.file("doc1")
file1.create("Hello World")

file2 = memory_fs.file("doc2")
file2.create("Hello World")

# Same content = same hash
assert file1.metadata().content__hash == file2.metadata().content__hash
```

### Storage Migration

Migrate between storage backends without data loss:

```python
# Source: SQLite
source_fs = Memory_FS__Latest()
source_fs.set_storage__sqlite(db_path="/old/data.db")

# Destination: S3
dest_fs = Memory_FS__Latest()
dest_fs.set_storage__s3(bucket="new-storage")

# Migration
for file_id in get_all_file_ids():
    source_file = source_fs.file(file_id)
    if source_file.exists():
        data = source_file.content()
        
        dest_file = dest_fs.file(file_id)
        dest_file.create(data)
        
        # Copy metadata
        dest_file.metadata__update(source_file.metadata().data)
```

### Atomic Operations

File operations are atomic at the storage layer:

```python
# SQLite: Transactions
sqlite_fs = Memory_FS()
sqlite_fs.set_storage__sqlite()

file = sqlite_fs.file("critical-data")
try:
    file.create(data)
    # Automatically committed on success
except Exception as e:
    # Automatically rolled back on failure
    print(f"Operation failed: {e}")
```

### Performance Optimization

Different backends for different access patterns:

```python
class OptimizedStorage:
    def __init__(self):
        # Hot data in memory
        self.hot = Memory_FS__Latest()
        self.hot.set_storage__memory()
        
        # Warm data in SQLite  
        self.warm = Memory_FS__Latest()
        self.warm.set_storage__sqlite(in_memory=True)
        
        # Cold data in S3
        self.cold = Memory_FS__Temporal()
        self.cold.set_storage__s3(bucket="archive")
    
    def store(self, file_id, data, tier="warm"):
        fs = getattr(self, tier)
        file = fs.file(file_id)
        return file.create(data)
```

## Type Safety Throughout

Every component uses Type_Safe for runtime validation:

```python
from osbot_utils.type_safe.Type_Safe import Type_Safe
from osbot_utils.type_safe.primitives.domains.files.safe_str.Safe_Str__File__Path import Safe_Str__File__Path
from osbot_utils.type_safe.primitives.domains.identifiers.safe_str.Safe_Str__Id import Safe_Str__Id

# File paths are validated
path = Safe_Str__File__Path("../../etc/passwd")  # Valid path format
# Result: "../../etc/passwd" (Safe_Str__File__Path ensures valid path format)

# File IDs are sanitized
file_id = Safe_Str__Id("my-file-123")  # Valid
file_id = Safe_Str__Id("my file!")     # Sanitized to "my_file_"

# This prevents:
# - Invalid file names
# - SQL injection via file IDs
# - Type confusion bugs
# - Malformed paths that could break storage backends
```

## Error Handling and Recovery

### Storage Failures

```python
class ResilientStorage:
    def save_with_retry(self, fs, file_id, data, retries=3):
        for attempt in range(retries):
            try:
                file = fs.file(file_id)
                return file.create(data)
            except StorageError as e:
                if attempt == retries - 1:
                    raise
                time.sleep(2 ** attempt)  # Exponential backoff
```

### Corruption Detection

```python
def verify_integrity(file):
    """Verify file hasn't been corrupted"""
    metadata = file.metadata()
    stored_hash = metadata.content__hash
    
    content = file.content()
    calculated_hash = calculate_hash(content)
    
    if stored_hash != calculated_hash:
        raise CorruptionError(f"File {file.file_id()} corrupted")
    
    return True
```

## Testing with Memory-FS

### Unit Testing

```python
from unittest import TestCase

class test_Document_Service(TestCase):
    @classmethod
    def setUpClass(cls):
        # Use in-memory storage for tests
        cls.fs = Memory_FS__Latest()
        cls.fs.set_storage__memory()
    
    def tearDown(self):
        # Clear between tests
        self.fs.storage_fs.clear()
    
    def test_save_document(self):
        file = self.fs.file("test-doc")
        file.create({"content": "test"})
        
        assert file.exists()
        assert file.content() == {"content": "test"}
```

### Integration Testing

```python
def test_s3_integration():
    """Test with LocalStack S3"""
    fs = Memory_FS__Latest()
    fs.set_storage__s3(
        bucket="test-bucket",
        endpoint_url="http://localhost:4566"  # LocalStack
    )
    
    file = fs.file("integration-test")
    file.create("test data")
    
    assert file.exists()
    assert file.content() == "test data"
```

## Performance Characteristics

### Operation Complexity

| Operation | Memory | SQLite | Local Disk | S3 | ZIP |
|-----------|--------|--------|------------|----|----|
| Create | O(1) | O(1) + I/O | O(1) + I/O | O(1) + Network | O(n) |
| Read | O(1) | O(1) + I/O | O(1) + I/O | O(1) + Network | O(n) |
| Update | O(1) | O(1) + I/O | O(1) + I/O | O(1) + Network | O(n) |
| Delete | O(1) | O(1) + I/O | O(1) + I/O | O(1) + Network | O(n) |
| List | O(n) | O(n) | O(n) | O(n) + Pagination | O(1) |

### Storage Overhead

Each file stores three components:
1. **Content**: Actual data (varies)
2. **Config**: ~200 bytes (JSON)
3. **Metadata**: ~500 bytes (JSON with hash, size, timestamp)

Total overhead: ~700 bytes per file plus path storage

## Summary

Memory-FS represents a fundamental rethinking of file storage abstractions. By separating WHERE files are stored (Storage_FS) from HOW they're organized (Path Handlers) and WHAT they contain (File Types), it provides unprecedented flexibility while maintaining simplicity.

The key insights that drove Memory-FS design:

1. **Storage and organization are orthogonal concerns** - Whether you store in S3 or SQLite is independent of whether you organize by time or version. Traditional file systems conflate these concerns.

2. **Multiple organizational views are often needed** - Real applications need the latest version for speed AND historical versions for auditing. Memory-FS provides both without complex synchronization.

3. **Type safety prevents entire categories of bugs** - By using Type_Safe throughout, Memory-FS eliminates type confusion and the mutable default antipattern.

4. **Abstraction enables testability** - The same code that uses S3 in production can use memory storage in tests, making tests 100x faster without mocking.

5. **Files are more than bytes** - Every file has content, configuration, and metadata. By tracking all three, Memory-FS enables features like content addressing, integrity verification, and rich querying.

Whether you need simple key-value storage, complex versioned documents, or time-series audit logs, Memory-FS provides the building blocks to implement your requirements cleanly and safely. Its production-tested patterns handle the complexity so you can focus on your domain logic.

## Open Source

Memory-FS is an open source project available on PyPI:
- **PyPI Package**: https://pypi.org/project/memory-fs/
- **Installation**: `pip install memory-fs`
- **Current Version**: v0.29.0

The project is actively maintained and used in production systems for configuration management, audit logging, content-addressable storage, and data persistence across various industries.